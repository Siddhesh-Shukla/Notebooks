{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Sentiment Analysis: Movie reviews.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "To9ENLU90WGl",
        "outputId": "3701c359-4fff-47e4-da4c-df3146a9f884",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvFvBLJV0Dkv"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import ( confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import torch.optim as optim\n",
        "import transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if torch.cuda.is_available():\n",
        "#   dev = torch.device('cuda:0')\n",
        "# else:\n",
        "#   dev = torch.device('cpu')\n",
        "dev = torch.device('cpu')"
      ],
      "metadata": {
        "id": "ayc-gy4xq4n1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ-42fh0hjsF"
      },
      "source": [
        "## Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyoj29J24hPX"
      },
      "source": [
        "df = pd.read_csv('data.tsv', delimiter='\\t', header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTM3hOHW4hUY"
      },
      "source": [
        "batch_1 = df[:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "crifWAXWUoAo",
        "outputId": "e2160047-1472-4779-fff8-624b4b10dad6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dac8c3b1-909d-4a88-b5de-ba2128ecba54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a stirring , funny and finally transporting re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apparently reassembled from the cutting room f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they presume their audience wo n't sit still f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is a visually stunning rumination on love...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jonathan parker 's bartleby should have been t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1995</th>\n",
              "      <td>too bland and fustily tasteful to be truly pru...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1996</th>\n",
              "      <td>it does n't work as either</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1997</th>\n",
              "      <td>this one aims for the toilet and scores a dire...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1998</th>\n",
              "      <td>in the name of an allegedly inspiring and easi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1999</th>\n",
              "      <td>the movie is undone by a filmmaking methodolog...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2000 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dac8c3b1-909d-4a88-b5de-ba2128ecba54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dac8c3b1-909d-4a88-b5de-ba2128ecba54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dac8c3b1-909d-4a88-b5de-ba2128ecba54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                      0  1\n",
              "0     a stirring , funny and finally transporting re...  1\n",
              "1     apparently reassembled from the cutting room f...  0\n",
              "2     they presume their audience wo n't sit still f...  0\n",
              "3     this is a visually stunning rumination on love...  1\n",
              "4     jonathan parker 's bartleby should have been t...  1\n",
              "...                                                 ... ..\n",
              "1995  too bland and fustily tasteful to be truly pru...  0\n",
              "1996                         it does n't work as either  0\n",
              "1997  this one aims for the toilet and scores a dire...  0\n",
              "1998  in the name of an allegedly inspiring and easi...  0\n",
              "1999  the movie is undone by a filmmaking methodolog...  0\n",
              "\n",
              "[2000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGvcfcCP5xpZ",
        "outputId": "f3e8761e-d8b3-4f2e-f50a-9e6badcb7568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "batch_1[1].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    1041\n",
              "0     959\n",
              "Name: 1, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_MO08_KiAOb"
      },
      "source": [
        "## Loading Pre-trained BERT model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1InADgf5xm2",
        "outputId": "b6fa5c83-fffa-4d5a-8406-cdde0fee6f5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (transformers.DistilBertModel, transformers.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "# BERT:\n",
        "#model_class, tokenizer_class, pretrained_weights = (transformers.BertModel, transformers.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dg82ndBA5xlN"
      },
      "source": [
        "tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tokenized)\n",
        "for i in tokenized[:15].values:\n",
        "    print(len(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cguLhtnrVIca",
        "outputId": "28470215-d5ec-48b9-b844-2667bf014172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "16\n",
            "45\n",
            "22\n",
            "25\n",
            "21\n",
            "21\n",
            "17\n",
            "28\n",
            "7\n",
            "22\n",
            "24\n",
            "29\n",
            "16\n",
            "18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URn-DWJt5xhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b027fc7-84a0-4f33-c273-8ae8f060549f"
      },
      "source": [
        "# calc maximum length of tokenized sentence\n",
        "\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "        max_len = len(i)\n",
        "\n",
        "print(max_len)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Padding\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])"
      ],
      "metadata": {
        "id": "MloyXBI2Vld1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swUa26JRVNaM",
        "outputId": "de630e19-edab-4242-82b4-e00e46a32d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  101,  1037, 18385, ...,     0,     0,     0],\n",
              "       [  101,  4593,  2128, ...,     0,     0,     0],\n",
              "       [  101,  2027,  3653, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101,  2023,  2028, ...,     0,     0,     0],\n",
              "       [  101,  1999,  1996, ...,     0,     0,     0],\n",
              "       [  101,  1996,  3185, ...,     0,     0,     0]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdi7uXo95xeq",
        "outputId": "64cc908d-e453-431c-f9d7-bc831696fdce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array(padded).shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 59)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K_iGRNa_Ozc",
        "outputId": "254072ba-9960-4b22-b7ac-5e9f214f6c39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Masking: to tell BERT to ignore/mask added padding \n",
        "\n",
        "attention_mask = np.where(padded != 0, 1, 0)\n",
        "attention_mask.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 59)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39UVjAV56PJz"
      },
      "source": [
        "## Tokentzed Sentence => BERT => Sentence Embedding\n",
        "\n",
        "input_ids = torch.tensor(padded)  \n",
        "attention_mask = torch.tensor(attention_mask)\n",
        "\n",
        "with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9t60At16PVs"
      },
      "source": [
        "# [CLS] Sentence [SEP]\n",
        "# output corresponding to [CLS] token is embedding for entire sentence.\n",
        "# saving features corresponding to [CLS]\n",
        "\n",
        "features = last_hidden_states[0][:,0,:].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6xctdONZNp5",
        "outputId": "de9d247f-f533-4131-ccd4-8f31a5e32a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.21593437, -0.14028914,  0.00831094, ..., -0.13694856,\n",
              "         0.5867004 ,  0.20112717],\n",
              "       [-0.17262724, -0.14476171,  0.00223437, ..., -0.17442554,\n",
              "         0.21386446,  0.37197468],\n",
              "       [-0.05063341,  0.07203971, -0.02959722, ..., -0.07148952,\n",
              "         0.7185241 ,  0.26225477],\n",
              "       ...,\n",
              "       [-0.27829772, -0.2480361 ,  0.13585785, ..., -0.19039167,\n",
              "         0.13099585,  0.3497837 ],\n",
              "       [-0.03667716,  0.10638539, -0.0111102 , ..., -0.11206637,\n",
              "         0.4161945 ,  0.50338024],\n",
              "       [ 0.12402633,  0.01425167,  0.01038398, ..., -0.11606557,\n",
              "         0.5345913 ,  0.27495337]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features.shape"
      ],
      "metadata": {
        "id": "ARVwOP9LasWY",
        "outputId": "937d8717-d35a-4e7e-d727-b45e199c8cb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2000, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD3fX2yh6PTx"
      },
      "source": [
        "labels = batch_1[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaoEvM2evRx1"
      },
      "source": [
        "## Train Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddAqbkoU6PP9"
      },
      "source": [
        "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(train_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "1n4QVdqutT8R",
        "outputId": "3e8e2730-9a8b-4912-ea9f-1bd615193f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c29bdf8b-3991-406c-9bdb-1c2cff104bfc\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>728</th>\n",
              "      <th>729</th>\n",
              "      <th>730</th>\n",
              "      <th>731</th>\n",
              "      <th>732</th>\n",
              "      <th>733</th>\n",
              "      <th>734</th>\n",
              "      <th>735</th>\n",
              "      <th>736</th>\n",
              "      <th>737</th>\n",
              "      <th>738</th>\n",
              "      <th>739</th>\n",
              "      <th>740</th>\n",
              "      <th>741</th>\n",
              "      <th>742</th>\n",
              "      <th>743</th>\n",
              "      <th>744</th>\n",
              "      <th>745</th>\n",
              "      <th>746</th>\n",
              "      <th>747</th>\n",
              "      <th>748</th>\n",
              "      <th>749</th>\n",
              "      <th>750</th>\n",
              "      <th>751</th>\n",
              "      <th>752</th>\n",
              "      <th>753</th>\n",
              "      <th>754</th>\n",
              "      <th>755</th>\n",
              "      <th>756</th>\n",
              "      <th>757</th>\n",
              "      <th>758</th>\n",
              "      <th>759</th>\n",
              "      <th>760</th>\n",
              "      <th>761</th>\n",
              "      <th>762</th>\n",
              "      <th>763</th>\n",
              "      <th>764</th>\n",
              "      <th>765</th>\n",
              "      <th>766</th>\n",
              "      <th>767</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.107823</td>\n",
              "      <td>0.111220</td>\n",
              "      <td>0.007751</td>\n",
              "      <td>-0.186264</td>\n",
              "      <td>0.017974</td>\n",
              "      <td>0.030283</td>\n",
              "      <td>0.243291</td>\n",
              "      <td>0.338382</td>\n",
              "      <td>-0.062372</td>\n",
              "      <td>-0.382746</td>\n",
              "      <td>-0.058209</td>\n",
              "      <td>0.004015</td>\n",
              "      <td>0.079638</td>\n",
              "      <td>0.205572</td>\n",
              "      <td>-0.141916</td>\n",
              "      <td>0.373760</td>\n",
              "      <td>0.087848</td>\n",
              "      <td>0.146198</td>\n",
              "      <td>0.149237</td>\n",
              "      <td>-0.111292</td>\n",
              "      <td>-0.091705</td>\n",
              "      <td>-0.160435</td>\n",
              "      <td>0.071609</td>\n",
              "      <td>0.062883</td>\n",
              "      <td>0.062110</td>\n",
              "      <td>0.128250</td>\n",
              "      <td>0.008427</td>\n",
              "      <td>-0.196469</td>\n",
              "      <td>0.027979</td>\n",
              "      <td>-0.016148</td>\n",
              "      <td>0.124949</td>\n",
              "      <td>0.289490</td>\n",
              "      <td>-0.097533</td>\n",
              "      <td>-0.147253</td>\n",
              "      <td>-0.096288</td>\n",
              "      <td>-0.084171</td>\n",
              "      <td>0.073260</td>\n",
              "      <td>0.030870</td>\n",
              "      <td>0.191073</td>\n",
              "      <td>0.083149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.022878</td>\n",
              "      <td>0.104922</td>\n",
              "      <td>-0.235396</td>\n",
              "      <td>0.387556</td>\n",
              "      <td>-0.072939</td>\n",
              "      <td>-0.340098</td>\n",
              "      <td>-0.065424</td>\n",
              "      <td>-0.024178</td>\n",
              "      <td>-0.022620</td>\n",
              "      <td>-0.217506</td>\n",
              "      <td>0.183706</td>\n",
              "      <td>0.011744</td>\n",
              "      <td>0.369475</td>\n",
              "      <td>0.005822</td>\n",
              "      <td>0.223637</td>\n",
              "      <td>0.165307</td>\n",
              "      <td>0.156381</td>\n",
              "      <td>-0.197007</td>\n",
              "      <td>-0.013248</td>\n",
              "      <td>-0.065221</td>\n",
              "      <td>-0.089332</td>\n",
              "      <td>-0.189002</td>\n",
              "      <td>-0.005612</td>\n",
              "      <td>0.145734</td>\n",
              "      <td>-7.047712</td>\n",
              "      <td>-0.410940</td>\n",
              "      <td>-0.064804</td>\n",
              "      <td>-0.173091</td>\n",
              "      <td>-0.163102</td>\n",
              "      <td>-0.013241</td>\n",
              "      <td>0.030233</td>\n",
              "      <td>-0.039298</td>\n",
              "      <td>0.070220</td>\n",
              "      <td>0.008022</td>\n",
              "      <td>0.264552</td>\n",
              "      <td>0.034919</td>\n",
              "      <td>-0.051451</td>\n",
              "      <td>0.101448</td>\n",
              "      <td>0.284157</td>\n",
              "      <td>0.335191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.095286</td>\n",
              "      <td>0.077067</td>\n",
              "      <td>-0.264647</td>\n",
              "      <td>-0.044359</td>\n",
              "      <td>-0.067140</td>\n",
              "      <td>-0.112859</td>\n",
              "      <td>0.135131</td>\n",
              "      <td>0.302451</td>\n",
              "      <td>-0.067289</td>\n",
              "      <td>0.030090</td>\n",
              "      <td>0.020095</td>\n",
              "      <td>0.107327</td>\n",
              "      <td>0.021946</td>\n",
              "      <td>0.430170</td>\n",
              "      <td>0.062794</td>\n",
              "      <td>0.427362</td>\n",
              "      <td>-0.139562</td>\n",
              "      <td>0.027472</td>\n",
              "      <td>0.131419</td>\n",
              "      <td>-0.050279</td>\n",
              "      <td>-0.326841</td>\n",
              "      <td>-0.318972</td>\n",
              "      <td>0.196475</td>\n",
              "      <td>0.185676</td>\n",
              "      <td>-0.155374</td>\n",
              "      <td>0.060176</td>\n",
              "      <td>-0.012004</td>\n",
              "      <td>-0.067358</td>\n",
              "      <td>0.002503</td>\n",
              "      <td>0.127649</td>\n",
              "      <td>0.152596</td>\n",
              "      <td>-0.037731</td>\n",
              "      <td>-0.267700</td>\n",
              "      <td>-0.274499</td>\n",
              "      <td>-0.006970</td>\n",
              "      <td>-0.184752</td>\n",
              "      <td>0.054932</td>\n",
              "      <td>0.015867</td>\n",
              "      <td>0.109912</td>\n",
              "      <td>-0.000082</td>\n",
              "      <td>...</td>\n",
              "      <td>0.121164</td>\n",
              "      <td>-0.217894</td>\n",
              "      <td>-0.048373</td>\n",
              "      <td>0.191506</td>\n",
              "      <td>-0.125046</td>\n",
              "      <td>-0.204126</td>\n",
              "      <td>0.187196</td>\n",
              "      <td>-0.144931</td>\n",
              "      <td>-0.183426</td>\n",
              "      <td>-0.031838</td>\n",
              "      <td>-0.176233</td>\n",
              "      <td>0.192707</td>\n",
              "      <td>0.253268</td>\n",
              "      <td>0.159180</td>\n",
              "      <td>-0.051619</td>\n",
              "      <td>0.110963</td>\n",
              "      <td>0.467664</td>\n",
              "      <td>0.136564</td>\n",
              "      <td>-0.116687</td>\n",
              "      <td>-0.194654</td>\n",
              "      <td>-0.225855</td>\n",
              "      <td>0.060305</td>\n",
              "      <td>-0.182051</td>\n",
              "      <td>-0.094858</td>\n",
              "      <td>-5.753145</td>\n",
              "      <td>-0.430544</td>\n",
              "      <td>-0.096196</td>\n",
              "      <td>-0.037099</td>\n",
              "      <td>0.009764</td>\n",
              "      <td>-0.019659</td>\n",
              "      <td>-0.022370</td>\n",
              "      <td>-0.321931</td>\n",
              "      <td>0.071652</td>\n",
              "      <td>-0.239595</td>\n",
              "      <td>-0.114711</td>\n",
              "      <td>0.239941</td>\n",
              "      <td>0.107733</td>\n",
              "      <td>0.061932</td>\n",
              "      <td>0.629441</td>\n",
              "      <td>0.417959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.089606</td>\n",
              "      <td>-0.114452</td>\n",
              "      <td>0.157323</td>\n",
              "      <td>0.015657</td>\n",
              "      <td>0.152667</td>\n",
              "      <td>-0.352516</td>\n",
              "      <td>0.098448</td>\n",
              "      <td>0.380798</td>\n",
              "      <td>-0.114708</td>\n",
              "      <td>-0.244245</td>\n",
              "      <td>-0.086395</td>\n",
              "      <td>0.037757</td>\n",
              "      <td>-0.010880</td>\n",
              "      <td>0.320740</td>\n",
              "      <td>0.162972</td>\n",
              "      <td>0.101190</td>\n",
              "      <td>-0.046626</td>\n",
              "      <td>0.213162</td>\n",
              "      <td>0.219105</td>\n",
              "      <td>-0.027633</td>\n",
              "      <td>-0.113588</td>\n",
              "      <td>-0.099889</td>\n",
              "      <td>0.089314</td>\n",
              "      <td>0.099486</td>\n",
              "      <td>-0.135969</td>\n",
              "      <td>-0.216305</td>\n",
              "      <td>-0.175764</td>\n",
              "      <td>0.225866</td>\n",
              "      <td>0.068100</td>\n",
              "      <td>-0.020560</td>\n",
              "      <td>-0.018281</td>\n",
              "      <td>0.051542</td>\n",
              "      <td>-0.388599</td>\n",
              "      <td>-0.336958</td>\n",
              "      <td>0.086325</td>\n",
              "      <td>0.039984</td>\n",
              "      <td>0.068570</td>\n",
              "      <td>-0.054057</td>\n",
              "      <td>-0.093390</td>\n",
              "      <td>-0.070891</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031014</td>\n",
              "      <td>-0.044603</td>\n",
              "      <td>-0.085249</td>\n",
              "      <td>-0.020924</td>\n",
              "      <td>0.009540</td>\n",
              "      <td>-0.146324</td>\n",
              "      <td>-0.087281</td>\n",
              "      <td>-0.330616</td>\n",
              "      <td>-0.140934</td>\n",
              "      <td>-0.185638</td>\n",
              "      <td>-0.060687</td>\n",
              "      <td>0.150497</td>\n",
              "      <td>0.058385</td>\n",
              "      <td>0.254196</td>\n",
              "      <td>-0.014072</td>\n",
              "      <td>0.113893</td>\n",
              "      <td>0.395107</td>\n",
              "      <td>0.282438</td>\n",
              "      <td>0.174987</td>\n",
              "      <td>-0.053787</td>\n",
              "      <td>-0.397886</td>\n",
              "      <td>0.098765</td>\n",
              "      <td>-0.180523</td>\n",
              "      <td>0.072121</td>\n",
              "      <td>-6.962331</td>\n",
              "      <td>-0.279294</td>\n",
              "      <td>-0.121245</td>\n",
              "      <td>0.065999</td>\n",
              "      <td>-0.156699</td>\n",
              "      <td>-0.217195</td>\n",
              "      <td>-0.229150</td>\n",
              "      <td>-0.178881</td>\n",
              "      <td>0.234193</td>\n",
              "      <td>-0.139085</td>\n",
              "      <td>0.037097</td>\n",
              "      <td>0.246068</td>\n",
              "      <td>-0.012880</td>\n",
              "      <td>-0.071799</td>\n",
              "      <td>0.529332</td>\n",
              "      <td>0.380615</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.125375</td>\n",
              "      <td>-0.058545</td>\n",
              "      <td>-0.129429</td>\n",
              "      <td>-0.107628</td>\n",
              "      <td>-0.304105</td>\n",
              "      <td>-0.203190</td>\n",
              "      <td>0.226137</td>\n",
              "      <td>0.158711</td>\n",
              "      <td>-0.076678</td>\n",
              "      <td>-0.044398</td>\n",
              "      <td>-0.296696</td>\n",
              "      <td>-0.127843</td>\n",
              "      <td>-0.045425</td>\n",
              "      <td>0.259921</td>\n",
              "      <td>0.199458</td>\n",
              "      <td>0.340376</td>\n",
              "      <td>-0.091153</td>\n",
              "      <td>0.130662</td>\n",
              "      <td>0.087126</td>\n",
              "      <td>-0.264176</td>\n",
              "      <td>0.094333</td>\n",
              "      <td>0.073670</td>\n",
              "      <td>-0.106589</td>\n",
              "      <td>0.040658</td>\n",
              "      <td>-0.055048</td>\n",
              "      <td>0.025232</td>\n",
              "      <td>0.191902</td>\n",
              "      <td>-0.043672</td>\n",
              "      <td>0.064993</td>\n",
              "      <td>0.178928</td>\n",
              "      <td>0.262746</td>\n",
              "      <td>0.059840</td>\n",
              "      <td>-0.006262</td>\n",
              "      <td>-0.329158</td>\n",
              "      <td>0.232546</td>\n",
              "      <td>-0.277781</td>\n",
              "      <td>-0.035381</td>\n",
              "      <td>-0.086124</td>\n",
              "      <td>0.304522</td>\n",
              "      <td>0.010976</td>\n",
              "      <td>...</td>\n",
              "      <td>0.088285</td>\n",
              "      <td>0.213585</td>\n",
              "      <td>0.108151</td>\n",
              "      <td>-0.022180</td>\n",
              "      <td>-0.364827</td>\n",
              "      <td>-0.195048</td>\n",
              "      <td>-0.051387</td>\n",
              "      <td>-0.199094</td>\n",
              "      <td>-0.047412</td>\n",
              "      <td>-0.091227</td>\n",
              "      <td>-0.004511</td>\n",
              "      <td>0.192218</td>\n",
              "      <td>-0.063572</td>\n",
              "      <td>0.051961</td>\n",
              "      <td>0.028780</td>\n",
              "      <td>0.095687</td>\n",
              "      <td>0.621607</td>\n",
              "      <td>0.192603</td>\n",
              "      <td>-0.127793</td>\n",
              "      <td>0.048106</td>\n",
              "      <td>-0.290899</td>\n",
              "      <td>-0.117908</td>\n",
              "      <td>-0.098175</td>\n",
              "      <td>0.256368</td>\n",
              "      <td>-6.155948</td>\n",
              "      <td>-0.370273</td>\n",
              "      <td>-0.142302</td>\n",
              "      <td>-0.025362</td>\n",
              "      <td>-0.047146</td>\n",
              "      <td>-0.234720</td>\n",
              "      <td>0.103592</td>\n",
              "      <td>-0.125050</td>\n",
              "      <td>-0.055821</td>\n",
              "      <td>0.000998</td>\n",
              "      <td>0.037700</td>\n",
              "      <td>0.044798</td>\n",
              "      <td>-0.067623</td>\n",
              "      <td>0.081697</td>\n",
              "      <td>0.446625</td>\n",
              "      <td>0.445961</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.131456</td>\n",
              "      <td>0.003370</td>\n",
              "      <td>-0.001269</td>\n",
              "      <td>-0.048962</td>\n",
              "      <td>-0.183062</td>\n",
              "      <td>-0.152779</td>\n",
              "      <td>0.217920</td>\n",
              "      <td>0.235372</td>\n",
              "      <td>0.097770</td>\n",
              "      <td>-0.090909</td>\n",
              "      <td>-0.033670</td>\n",
              "      <td>-0.107399</td>\n",
              "      <td>-0.164156</td>\n",
              "      <td>0.628630</td>\n",
              "      <td>0.195057</td>\n",
              "      <td>0.367297</td>\n",
              "      <td>-0.146439</td>\n",
              "      <td>0.111491</td>\n",
              "      <td>0.172675</td>\n",
              "      <td>-0.236713</td>\n",
              "      <td>-0.058929</td>\n",
              "      <td>-0.183212</td>\n",
              "      <td>0.299020</td>\n",
              "      <td>0.155977</td>\n",
              "      <td>-0.165793</td>\n",
              "      <td>0.111152</td>\n",
              "      <td>0.065651</td>\n",
              "      <td>-0.026628</td>\n",
              "      <td>-0.079303</td>\n",
              "      <td>0.171861</td>\n",
              "      <td>0.125053</td>\n",
              "      <td>-0.121696</td>\n",
              "      <td>-0.276433</td>\n",
              "      <td>-0.254619</td>\n",
              "      <td>0.232073</td>\n",
              "      <td>-0.268342</td>\n",
              "      <td>-0.129371</td>\n",
              "      <td>0.057542</td>\n",
              "      <td>0.080998</td>\n",
              "      <td>-0.122877</td>\n",
              "      <td>...</td>\n",
              "      <td>0.036485</td>\n",
              "      <td>-0.124530</td>\n",
              "      <td>0.070041</td>\n",
              "      <td>0.013081</td>\n",
              "      <td>-0.248389</td>\n",
              "      <td>-0.204709</td>\n",
              "      <td>0.088085</td>\n",
              "      <td>-0.245095</td>\n",
              "      <td>0.023698</td>\n",
              "      <td>-0.166536</td>\n",
              "      <td>-0.107585</td>\n",
              "      <td>0.232298</td>\n",
              "      <td>0.110611</td>\n",
              "      <td>0.285307</td>\n",
              "      <td>0.102113</td>\n",
              "      <td>0.280949</td>\n",
              "      <td>0.367623</td>\n",
              "      <td>0.122324</td>\n",
              "      <td>0.011049</td>\n",
              "      <td>-0.145418</td>\n",
              "      <td>-0.296109</td>\n",
              "      <td>-0.057227</td>\n",
              "      <td>-0.062689</td>\n",
              "      <td>0.102168</td>\n",
              "      <td>-6.480539</td>\n",
              "      <td>-0.287889</td>\n",
              "      <td>-0.188958</td>\n",
              "      <td>0.200361</td>\n",
              "      <td>0.131083</td>\n",
              "      <td>-0.089436</td>\n",
              "      <td>0.108468</td>\n",
              "      <td>-0.222599</td>\n",
              "      <td>0.006665</td>\n",
              "      <td>-0.124408</td>\n",
              "      <td>-0.028487</td>\n",
              "      <td>0.139138</td>\n",
              "      <td>0.076367</td>\n",
              "      <td>-0.151278</td>\n",
              "      <td>0.527354</td>\n",
              "      <td>0.315475</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>0.079769</td>\n",
              "      <td>0.163821</td>\n",
              "      <td>0.039639</td>\n",
              "      <td>-0.200538</td>\n",
              "      <td>-0.105984</td>\n",
              "      <td>0.060898</td>\n",
              "      <td>0.349350</td>\n",
              "      <td>0.226700</td>\n",
              "      <td>-0.142239</td>\n",
              "      <td>-0.047896</td>\n",
              "      <td>0.060450</td>\n",
              "      <td>-0.027002</td>\n",
              "      <td>-0.013967</td>\n",
              "      <td>0.204028</td>\n",
              "      <td>0.049045</td>\n",
              "      <td>0.198704</td>\n",
              "      <td>0.005393</td>\n",
              "      <td>0.047892</td>\n",
              "      <td>0.163269</td>\n",
              "      <td>-0.087299</td>\n",
              "      <td>0.031308</td>\n",
              "      <td>-0.189562</td>\n",
              "      <td>-0.055707</td>\n",
              "      <td>0.022088</td>\n",
              "      <td>-0.044758</td>\n",
              "      <td>0.103614</td>\n",
              "      <td>-0.038133</td>\n",
              "      <td>0.024916</td>\n",
              "      <td>0.048381</td>\n",
              "      <td>-0.082992</td>\n",
              "      <td>-0.015532</td>\n",
              "      <td>0.015227</td>\n",
              "      <td>-0.103904</td>\n",
              "      <td>-0.085219</td>\n",
              "      <td>-0.112678</td>\n",
              "      <td>-0.071668</td>\n",
              "      <td>0.013599</td>\n",
              "      <td>-0.061171</td>\n",
              "      <td>0.221332</td>\n",
              "      <td>-0.086875</td>\n",
              "      <td>...</td>\n",
              "      <td>0.099521</td>\n",
              "      <td>-0.071422</td>\n",
              "      <td>-0.127484</td>\n",
              "      <td>0.269602</td>\n",
              "      <td>-0.048991</td>\n",
              "      <td>-0.161537</td>\n",
              "      <td>0.075697</td>\n",
              "      <td>-0.106613</td>\n",
              "      <td>-0.017382</td>\n",
              "      <td>-0.096197</td>\n",
              "      <td>-0.100574</td>\n",
              "      <td>0.217639</td>\n",
              "      <td>0.068109</td>\n",
              "      <td>-0.114135</td>\n",
              "      <td>-0.156305</td>\n",
              "      <td>0.201475</td>\n",
              "      <td>0.221744</td>\n",
              "      <td>-0.028822</td>\n",
              "      <td>0.024932</td>\n",
              "      <td>0.030958</td>\n",
              "      <td>-0.162845</td>\n",
              "      <td>-0.133049</td>\n",
              "      <td>-0.142279</td>\n",
              "      <td>0.231040</td>\n",
              "      <td>-7.189999</td>\n",
              "      <td>-0.116854</td>\n",
              "      <td>0.120087</td>\n",
              "      <td>-0.271360</td>\n",
              "      <td>-0.094423</td>\n",
              "      <td>-0.066835</td>\n",
              "      <td>-0.016034</td>\n",
              "      <td>-0.194708</td>\n",
              "      <td>0.071258</td>\n",
              "      <td>-0.131594</td>\n",
              "      <td>0.261649</td>\n",
              "      <td>-0.031568</td>\n",
              "      <td>-0.192514</td>\n",
              "      <td>0.069214</td>\n",
              "      <td>0.291201</td>\n",
              "      <td>0.246542</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1496</th>\n",
              "      <td>-0.104785</td>\n",
              "      <td>-0.029085</td>\n",
              "      <td>-0.203088</td>\n",
              "      <td>-0.148878</td>\n",
              "      <td>-0.261948</td>\n",
              "      <td>-0.071748</td>\n",
              "      <td>0.223727</td>\n",
              "      <td>0.322300</td>\n",
              "      <td>-0.041509</td>\n",
              "      <td>-0.365323</td>\n",
              "      <td>-0.010407</td>\n",
              "      <td>0.044167</td>\n",
              "      <td>-0.346317</td>\n",
              "      <td>0.722577</td>\n",
              "      <td>0.274255</td>\n",
              "      <td>0.287661</td>\n",
              "      <td>-0.160171</td>\n",
              "      <td>0.251146</td>\n",
              "      <td>0.126535</td>\n",
              "      <td>-0.052594</td>\n",
              "      <td>-0.296413</td>\n",
              "      <td>-0.315045</td>\n",
              "      <td>0.095475</td>\n",
              "      <td>0.035743</td>\n",
              "      <td>0.034613</td>\n",
              "      <td>0.044680</td>\n",
              "      <td>-0.160295</td>\n",
              "      <td>-0.017160</td>\n",
              "      <td>-0.029478</td>\n",
              "      <td>0.357034</td>\n",
              "      <td>0.042105</td>\n",
              "      <td>0.063991</td>\n",
              "      <td>-0.219876</td>\n",
              "      <td>-0.144120</td>\n",
              "      <td>-0.025838</td>\n",
              "      <td>-0.151011</td>\n",
              "      <td>0.059953</td>\n",
              "      <td>-0.042674</td>\n",
              "      <td>0.062193</td>\n",
              "      <td>0.080262</td>\n",
              "      <td>...</td>\n",
              "      <td>0.129053</td>\n",
              "      <td>-0.015363</td>\n",
              "      <td>0.109720</td>\n",
              "      <td>0.540686</td>\n",
              "      <td>-0.251908</td>\n",
              "      <td>-0.350180</td>\n",
              "      <td>0.077148</td>\n",
              "      <td>0.052013</td>\n",
              "      <td>-0.009537</td>\n",
              "      <td>-0.103565</td>\n",
              "      <td>-0.183218</td>\n",
              "      <td>0.004490</td>\n",
              "      <td>0.272250</td>\n",
              "      <td>0.142626</td>\n",
              "      <td>0.106060</td>\n",
              "      <td>0.210836</td>\n",
              "      <td>0.060382</td>\n",
              "      <td>-0.054857</td>\n",
              "      <td>0.042420</td>\n",
              "      <td>-0.153189</td>\n",
              "      <td>-0.240065</td>\n",
              "      <td>0.072518</td>\n",
              "      <td>-0.196466</td>\n",
              "      <td>0.116762</td>\n",
              "      <td>-7.326873</td>\n",
              "      <td>-0.225119</td>\n",
              "      <td>-0.369815</td>\n",
              "      <td>-0.242827</td>\n",
              "      <td>0.162992</td>\n",
              "      <td>0.114415</td>\n",
              "      <td>0.028137</td>\n",
              "      <td>-0.052976</td>\n",
              "      <td>0.071522</td>\n",
              "      <td>-0.063701</td>\n",
              "      <td>0.159390</td>\n",
              "      <td>-0.146770</td>\n",
              "      <td>0.063661</td>\n",
              "      <td>-0.195279</td>\n",
              "      <td>0.318506</td>\n",
              "      <td>0.181844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1497</th>\n",
              "      <td>-0.212149</td>\n",
              "      <td>-0.076728</td>\n",
              "      <td>0.055190</td>\n",
              "      <td>-0.233146</td>\n",
              "      <td>-0.020902</td>\n",
              "      <td>0.041325</td>\n",
              "      <td>0.141727</td>\n",
              "      <td>0.335852</td>\n",
              "      <td>0.155386</td>\n",
              "      <td>-0.159949</td>\n",
              "      <td>0.031903</td>\n",
              "      <td>-0.030581</td>\n",
              "      <td>-0.009845</td>\n",
              "      <td>0.522848</td>\n",
              "      <td>-0.006941</td>\n",
              "      <td>0.208238</td>\n",
              "      <td>0.119222</td>\n",
              "      <td>0.154942</td>\n",
              "      <td>0.169346</td>\n",
              "      <td>-0.096435</td>\n",
              "      <td>-0.066246</td>\n",
              "      <td>-0.357769</td>\n",
              "      <td>-0.153195</td>\n",
              "      <td>0.123195</td>\n",
              "      <td>0.091804</td>\n",
              "      <td>0.021859</td>\n",
              "      <td>0.178684</td>\n",
              "      <td>0.016428</td>\n",
              "      <td>0.154030</td>\n",
              "      <td>0.129672</td>\n",
              "      <td>0.099427</td>\n",
              "      <td>0.140332</td>\n",
              "      <td>-0.075095</td>\n",
              "      <td>-0.233111</td>\n",
              "      <td>-0.144989</td>\n",
              "      <td>-0.102034</td>\n",
              "      <td>0.176075</td>\n",
              "      <td>-0.128918</td>\n",
              "      <td>-0.024960</td>\n",
              "      <td>0.157065</td>\n",
              "      <td>...</td>\n",
              "      <td>0.013512</td>\n",
              "      <td>0.122073</td>\n",
              "      <td>0.189287</td>\n",
              "      <td>0.246285</td>\n",
              "      <td>-0.242550</td>\n",
              "      <td>-0.306299</td>\n",
              "      <td>0.127041</td>\n",
              "      <td>0.094134</td>\n",
              "      <td>0.120842</td>\n",
              "      <td>-0.234104</td>\n",
              "      <td>-0.164013</td>\n",
              "      <td>0.313328</td>\n",
              "      <td>0.283877</td>\n",
              "      <td>0.276368</td>\n",
              "      <td>0.026969</td>\n",
              "      <td>0.235984</td>\n",
              "      <td>0.312396</td>\n",
              "      <td>-0.002278</td>\n",
              "      <td>-0.023461</td>\n",
              "      <td>0.019588</td>\n",
              "      <td>-0.270537</td>\n",
              "      <td>-0.053531</td>\n",
              "      <td>-0.039019</td>\n",
              "      <td>0.161347</td>\n",
              "      <td>-7.209646</td>\n",
              "      <td>-0.058369</td>\n",
              "      <td>-0.134648</td>\n",
              "      <td>-0.114268</td>\n",
              "      <td>-0.014603</td>\n",
              "      <td>-0.074837</td>\n",
              "      <td>0.073483</td>\n",
              "      <td>-0.197092</td>\n",
              "      <td>0.006387</td>\n",
              "      <td>-0.078687</td>\n",
              "      <td>0.269555</td>\n",
              "      <td>-0.224290</td>\n",
              "      <td>-0.026730</td>\n",
              "      <td>-0.120650</td>\n",
              "      <td>0.325458</td>\n",
              "      <td>0.316969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1498</th>\n",
              "      <td>-0.186114</td>\n",
              "      <td>-0.078680</td>\n",
              "      <td>-0.135280</td>\n",
              "      <td>-0.368205</td>\n",
              "      <td>-0.294796</td>\n",
              "      <td>-0.282086</td>\n",
              "      <td>0.090594</td>\n",
              "      <td>0.345995</td>\n",
              "      <td>-0.082809</td>\n",
              "      <td>-0.074644</td>\n",
              "      <td>0.164646</td>\n",
              "      <td>-0.187514</td>\n",
              "      <td>-0.226415</td>\n",
              "      <td>0.309187</td>\n",
              "      <td>-0.084051</td>\n",
              "      <td>0.120422</td>\n",
              "      <td>-0.027516</td>\n",
              "      <td>0.127738</td>\n",
              "      <td>0.104486</td>\n",
              "      <td>0.000606</td>\n",
              "      <td>-0.163169</td>\n",
              "      <td>-0.125617</td>\n",
              "      <td>0.318097</td>\n",
              "      <td>-0.029062</td>\n",
              "      <td>-0.008502</td>\n",
              "      <td>-0.217723</td>\n",
              "      <td>0.147262</td>\n",
              "      <td>0.175482</td>\n",
              "      <td>-0.012480</td>\n",
              "      <td>0.126878</td>\n",
              "      <td>0.088208</td>\n",
              "      <td>0.024506</td>\n",
              "      <td>-0.386643</td>\n",
              "      <td>-0.180911</td>\n",
              "      <td>0.067882</td>\n",
              "      <td>-0.040278</td>\n",
              "      <td>0.321595</td>\n",
              "      <td>0.208379</td>\n",
              "      <td>0.074737</td>\n",
              "      <td>0.146272</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.006339</td>\n",
              "      <td>-0.232416</td>\n",
              "      <td>0.128490</td>\n",
              "      <td>0.145906</td>\n",
              "      <td>0.073259</td>\n",
              "      <td>-0.289090</td>\n",
              "      <td>0.126051</td>\n",
              "      <td>-0.230831</td>\n",
              "      <td>-0.117151</td>\n",
              "      <td>-0.242391</td>\n",
              "      <td>-0.042123</td>\n",
              "      <td>0.129204</td>\n",
              "      <td>0.253589</td>\n",
              "      <td>0.076780</td>\n",
              "      <td>-0.012842</td>\n",
              "      <td>0.643095</td>\n",
              "      <td>0.449543</td>\n",
              "      <td>0.353295</td>\n",
              "      <td>0.177948</td>\n",
              "      <td>-0.083902</td>\n",
              "      <td>-0.078520</td>\n",
              "      <td>0.134436</td>\n",
              "      <td>0.026527</td>\n",
              "      <td>0.035280</td>\n",
              "      <td>-6.210321</td>\n",
              "      <td>-0.239505</td>\n",
              "      <td>-0.397245</td>\n",
              "      <td>0.087502</td>\n",
              "      <td>0.129845</td>\n",
              "      <td>-0.079933</td>\n",
              "      <td>0.041359</td>\n",
              "      <td>-0.330706</td>\n",
              "      <td>0.212895</td>\n",
              "      <td>-0.111953</td>\n",
              "      <td>0.010916</td>\n",
              "      <td>0.101088</td>\n",
              "      <td>-0.052374</td>\n",
              "      <td>-0.288308</td>\n",
              "      <td>0.417482</td>\n",
              "      <td>0.191375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1499</th>\n",
              "      <td>-0.220794</td>\n",
              "      <td>-0.196620</td>\n",
              "      <td>-0.146692</td>\n",
              "      <td>-0.060718</td>\n",
              "      <td>-0.102955</td>\n",
              "      <td>-0.055626</td>\n",
              "      <td>0.182195</td>\n",
              "      <td>-0.028424</td>\n",
              "      <td>-0.058269</td>\n",
              "      <td>-0.021800</td>\n",
              "      <td>0.027965</td>\n",
              "      <td>-0.073304</td>\n",
              "      <td>0.134912</td>\n",
              "      <td>0.353547</td>\n",
              "      <td>-0.083458</td>\n",
              "      <td>0.313681</td>\n",
              "      <td>-0.057695</td>\n",
              "      <td>0.169133</td>\n",
              "      <td>0.166429</td>\n",
              "      <td>-0.304613</td>\n",
              "      <td>-0.136749</td>\n",
              "      <td>-0.104015</td>\n",
              "      <td>0.162493</td>\n",
              "      <td>0.215057</td>\n",
              "      <td>-0.237949</td>\n",
              "      <td>0.139721</td>\n",
              "      <td>-0.060089</td>\n",
              "      <td>-0.035059</td>\n",
              "      <td>0.052627</td>\n",
              "      <td>0.041055</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.047805</td>\n",
              "      <td>-0.270400</td>\n",
              "      <td>-0.107449</td>\n",
              "      <td>0.023371</td>\n",
              "      <td>-0.108360</td>\n",
              "      <td>0.035559</td>\n",
              "      <td>0.057234</td>\n",
              "      <td>0.176290</td>\n",
              "      <td>-0.132758</td>\n",
              "      <td>...</td>\n",
              "      <td>0.049403</td>\n",
              "      <td>-0.065684</td>\n",
              "      <td>0.137177</td>\n",
              "      <td>0.122908</td>\n",
              "      <td>-0.209292</td>\n",
              "      <td>-0.016289</td>\n",
              "      <td>-0.174030</td>\n",
              "      <td>-0.317312</td>\n",
              "      <td>0.197864</td>\n",
              "      <td>-0.147421</td>\n",
              "      <td>0.068416</td>\n",
              "      <td>0.252100</td>\n",
              "      <td>0.117856</td>\n",
              "      <td>0.061334</td>\n",
              "      <td>0.080336</td>\n",
              "      <td>0.228181</td>\n",
              "      <td>0.116421</td>\n",
              "      <td>-0.001780</td>\n",
              "      <td>-0.059921</td>\n",
              "      <td>-0.020751</td>\n",
              "      <td>-0.256300</td>\n",
              "      <td>-0.072619</td>\n",
              "      <td>-0.165042</td>\n",
              "      <td>0.196861</td>\n",
              "      <td>-6.484745</td>\n",
              "      <td>-0.241241</td>\n",
              "      <td>-0.076837</td>\n",
              "      <td>-0.015978</td>\n",
              "      <td>0.073664</td>\n",
              "      <td>-0.161498</td>\n",
              "      <td>-0.037750</td>\n",
              "      <td>-0.273844</td>\n",
              "      <td>0.088845</td>\n",
              "      <td>-0.049832</td>\n",
              "      <td>-0.095877</td>\n",
              "      <td>0.105693</td>\n",
              "      <td>0.140095</td>\n",
              "      <td>-0.028362</td>\n",
              "      <td>0.397386</td>\n",
              "      <td>0.253097</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1500 rows Ã— 768 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c29bdf8b-3991-406c-9bdb-1c2cff104bfc')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c29bdf8b-3991-406c-9bdb-1c2cff104bfc button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c29bdf8b-3991-406c-9bdb-1c2cff104bfc');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "           0         1         2    ...       765       766       767\n",
              "0    -0.107823  0.111220  0.007751  ...  0.101448  0.284157  0.335191\n",
              "1     0.095286  0.077067 -0.264647  ...  0.061932  0.629441  0.417959\n",
              "2     0.089606 -0.114452  0.157323  ... -0.071799  0.529332  0.380615\n",
              "3    -0.125375 -0.058545 -0.129429  ...  0.081697  0.446625  0.445961\n",
              "4     0.131456  0.003370 -0.001269  ... -0.151278  0.527354  0.315475\n",
              "...        ...       ...       ...  ...       ...       ...       ...\n",
              "1495  0.079769  0.163821  0.039639  ...  0.069214  0.291201  0.246542\n",
              "1496 -0.104785 -0.029085 -0.203088  ... -0.195279  0.318506  0.181844\n",
              "1497 -0.212149 -0.076728  0.055190  ... -0.120650  0.325458  0.316969\n",
              "1498 -0.186114 -0.078680 -0.135280  ... -0.288308  0.417482  0.191375\n",
              "1499 -0.220794 -0.196620 -0.146692  ... -0.028362  0.397386  0.253097\n",
              "\n",
              "[1500 rows x 768 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = pd.concat([train_features, train_labels], axis=1, join=\"inner\")\n",
        "test_dataloader = pd.concat([test_features, test_labels], axis=1, join=\"inner\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "xUMFoL-ss8Ql",
        "outputId": "8b597f89-042d-456b-8220-95d7d4ffdc93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-2e4a9ee19a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;34m\"only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 )\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gG-EVWx4CzBc",
        "outputId": "72eb6cb5-61ae-4455-f22b-c3b54fea48ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(train_features, train_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression()"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCoyxRJ7ECTA",
        "outputId": "915ebabc-d6fa-4072-b609-5552a4bc15cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.score(test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.846"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJQuqV6cnWQu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment = {0:\"Negative\", 1:\"Positive\"}\n",
        "\n",
        "# Dataset Parameters\n",
        "input_nodes = 768\n",
        "classes = 2\n",
        "batch_size = 32\n"
      ],
      "metadata": {
        "id": "uDIRN-1wrQIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(data_loader, model, is_test):\n",
        "    correct = 0\n",
        "    samples = 0\n",
        "    cm = np.zeros((10, 10))\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data_features, labels in data_loader:\n",
        "            data_features = data_features.to(device=dev)\n",
        "            labels = labels.to(device=dev)\n",
        "            data_features = data_features.reshape(data_features.shape[0], -1)\n",
        "\n",
        "            outputs = model(data_features)\n",
        "            _, predictions = outputs.max(1)\n",
        "            correct += (predictions == labels).sum()\n",
        "            samples += predictions.size(0)\n",
        "            acc = correct/samples\n",
        "            if is_test:\n",
        "              cm1 = confusion_matrix(predictions.cpu().numpy(),labels.cpu().numpy())\n",
        "              if cm1.shape == (10, 10):\n",
        "                cm += cm1\n",
        "\n",
        "    model.train()\n",
        "    return acc, cm"
      ],
      "metadata": {
        "id": "vpoJw2Farn1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Network\n",
        "def trainer(model, train_dataloader, test_dataloader, epochs, loss_fn, optimizer):\n",
        "  \n",
        "  losses = []\n",
        "  accuracies = []\n",
        "  test_acc = 0.0\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    samples = 0\n",
        "    for batch_id, (data, labels) in enumerate(tqdm(train_dataloader)):\n",
        "        \n",
        "        # Use cuda\n",
        "        data = data.to(device=dev)\n",
        "        labels = labels.to(device=dev)\n",
        "\n",
        "        # Reshape to flatten\n",
        "        data = data.reshape(data.shape[0], -1)\n",
        "\n",
        "        # forward\n",
        "        outputs = model(data)\n",
        "        if isinstance(loss_fn, nn.NLLLoss):\n",
        "          outputs = f.log_softmax(outputs,dim=1)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        total_loss += loss.item()*outputs.size(0)\n",
        "        samples += outputs.size(0)\n",
        "\n",
        "        # calculate gradients of loss function\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # adam step or gradient descent\n",
        "        optimizer.step()\n",
        "    \n",
        "    acc, _ = accuracy(train_dataloader, model, False)\n",
        "    avg_loss = total_loss/samples\n",
        "\n",
        "    losses.append(avg_loss)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "    print(f\"Epoch: {epoch+1} | Accuracy on training set: {acc*100:.2f} | Loss: {avg_loss:.5f}\")\n",
        "\n",
        "  test_acc, cm = accuracy(test_dataloader, model, True)\n",
        "\n",
        "  return losses, accuracies, test_acc, cm"
      ],
      "metadata": {
        "id": "B3vz5KsErtBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_graphs(losses, accuracies, epochs, cm):\n",
        "  x = np.array(range(1,epochs+1))\n",
        "\n",
        "  losses = np.array(losses)\n",
        "\n",
        "  fig = plt.figure()\n",
        "  plt.plot(x, losses)\n",
        "  fig.suptitle('Loss Vs. Epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xticks(x)\n",
        "  plt.show()\n",
        "\n",
        "  accuracies = np.array(accuracies)*100\n",
        "\n",
        "\n",
        "  fig = plt.figure()\n",
        "  plt.plot(x, accuracies)\n",
        "  fig.suptitle('Accuracy Vs. Epochs')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy (in %)')\n",
        "  plt.xticks(x)\n",
        "  plt.show()\n",
        "\n",
        "  cmd = ConfusionMatrixDisplay(cm, display_labels=[label for key, label in clothes.items()])\n",
        "  fig, ax = plt.subplots(figsize=(10,10))\n",
        "  cmd.plot(ax=ax)"
      ],
      "metadata": {
        "id": "HEfpeGddrtEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(cm):\n",
        "  true_positive = cm.diagonal()\n",
        "  false_positive = cm.sum(0) - true_positive\n",
        "  false_negative = cm.sum(1) - true_positive\n",
        "\n",
        "  precision = true_positive / (true_positive + false_positive)\n",
        "  recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "  f_score = 2*(precision*recall)/(precision+recall)\n",
        "  return precision, recall, f_score"
      ],
      "metadata": {
        "id": "CmPFdXrfrz4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_accuracies = []"
      ],
      "metadata": {
        "id": "l4TT7pbxrz7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# KL Divergence Model and its parameters\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.hidden_layer1 = nn.Linear(input_size, 16)\n",
        "        self.output_layer = nn.Linear(16, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Activation Functions\n",
        "        x = f.relu(self.hidden_layer1(x))\n",
        "        x = torch.sigmoid(self.output_layer(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Initialize network\n",
        "model = NeuralNet(input_size=input_nodes, num_classes=classes).to(dev)\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 2\n",
        "learning_rate = 0.0001\n",
        "loss_fn = nn.NLLLoss()   # Loss Function\n",
        "# loss_fn = nn.CrossEntropyLoss()    # Loss Function\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Results\n",
        "losses, accuracies, test_acc, cm = trainer(model, train_dataloader, test_dataloader, epochs, loss_fn, optimizer)\n",
        "p, r, fs = metrics(cm)\n",
        "\n",
        "print(\"-\"*10 + \"Test Results\" + \"-\"*10)\n",
        "print(f\"Accuracy on test set: {test_acc*100:.2f}\")\n",
        "print(f\"Precision: {p}\")\n",
        "print(f\"Recall: {r}\")\n",
        "print(f\"F1 score: {fs}\")\n",
        "\n",
        "test_accuracies.append(test_acc)\n",
        "\n",
        "draw_graphs(losses, accuracies, epochs, cm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "HajfJhUMrz9y",
        "outputId": "bf81d654-4fd9-4681-8c5c-102f0a6c1112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/1500 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-72516fe2cbc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-ea969b18ed59>\u001b[0m in \u001b[0;36mtrainer\u001b[0;34m(model, train_dataloader, test_dataloader, epochs, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Use cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
          ]
        }
      ]
    }
  ]
}